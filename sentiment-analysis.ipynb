{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the datasets\n",
    "# Import the training dataset with only the important colums\n",
    "train_df = pd.read_csv(\"datasets/train.csv\")\n",
    "\n",
    "# Column 3 is the column of interest in the testing set\n",
    "test_df = pd.read_csv(\"datasets/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explore train dataset\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment\n",
       "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral\n",
       "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive\n",
       "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative\n",
       "3  01082688c6                                        happy bday!  positive\n",
       "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the dataset\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove links and non-alphanumeric characters, and convert to lowercase\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text.lower())\n",
    "        text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "\n",
    "        # Tokenize the text into individual words\n",
    "        words = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "\n",
    "        # Join the words back into a single string\n",
    "        clean_text = ' '.join(words)\n",
    "\n",
    "        return clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before dropna: 27481\n",
      "Number of rows after dropna: 27480\n",
      "       textID                                               text  \\\n",
      "0  cb774db0d1                I`d have responded, if I were going   \n",
      "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
      "2  088c60f138                          my boss is bullying me...   \n",
      "3  9642c003ef                     what interview! leave me alone   \n",
      "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
      "\n",
      "                         selected_text  sentiment  \\\n",
      "0  I`d have responded, if I were going          0   \n",
      "1                             Sooo SAD         -1   \n",
      "2                          bullying me         -1   \n",
      "3                       leave me alone         -1   \n",
      "4                        Sons of ****,         -1   \n",
      "\n",
      "                       cleaned_text  \n",
      "0                   responded going  \n",
      "1           sooo sad miss san diego  \n",
      "2                     boss bullying  \n",
      "3             interview leave alone  \n",
      "4  sons put releases already bought  \n"
     ]
    }
   ],
   "source": [
    "# Applying preprocessing function on the text column of the training dataset\n",
    "train_df['cleaned_text'] = train_df['text'].apply(clean_text)\n",
    "\n",
    "# Converting sentiments to numerical form\n",
    "sentiment_mapping = {'neutral': 0, 'positive': 1, 'negative': -1}\n",
    "train_df['sentiment'] = train_df['sentiment'].map(sentiment_mapping)\n",
    "print(\"Number of rows before dropna:\", train_df.shape[0])\n",
    "\n",
    "# Dropping rows with missing values\n",
    "train_df.dropna(inplace=True)\n",
    "print(\"Number of rows after dropna:\", train_df.shape[0])\n",
    "\n",
    "# Print the preprocessed train dataset\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the training dataset into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df.cleaned_text, train_df.sentiment, test_size=0.2, random_state=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:558: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialising the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1)\n",
    "\n",
    "vectorizer.fit(list(X_train) + list(X_test))\n",
    "X_train_vectorizer =  vectorizer.transform(X_train) \n",
    "X_test_vectorizer = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;multinomial&#x27;, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;multinomial&#x27;, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, multi_class='multinomial', random_state=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state=0, multi_class='multinomial', max_iter=1000)\n",
    "log_reg.fit(X_train_vectorizer, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accurary of logistic regression is 0.6848617176128093\n"
     ]
    }
   ],
   "source": [
    "val_pred = log_reg.predict(X_test_vectorizer)\n",
    "print(\"The accurary of logistic regression is\",accuracy_score(list(y_test), val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0        00       000        01        03        04        05  \\\n",
      "0 -0.029928  0.536696 -0.226815  0.062376  0.254931  0.227530  0.138556   \n",
      "1  0.153810 -0.099758 -0.069210  0.080082  0.043592 -0.164294 -0.146658   \n",
      "2 -0.123882 -0.436938  0.296025 -0.142458 -0.298522 -0.063236  0.008101   \n",
      "\n",
      "         06        07        08  ...        ze   zealand      zero       zoe  \\\n",
      "0  0.313694 -0.118439 -0.247789  ...  0.098485  0.003092 -0.380616 -0.182633   \n",
      "1 -0.034245  0.251853  0.206534  ... -0.178707 -0.140191  0.384439  0.235802   \n",
      "2 -0.279449 -0.133413  0.041255  ...  0.080222  0.137099 -0.003823 -0.053169   \n",
      "\n",
      "     zombie   zombies      zone      zoo      zulu      zzzz  \n",
      "0  0.099316  0.071882  0.012219 -0.33609 -0.310911  0.010866  \n",
      "1 -0.035414 -0.071920  0.272654  0.25814 -0.006256 -0.099284  \n",
      "2 -0.063902  0.000038 -0.284874  0.07795  0.317167  0.088418  \n",
      "\n",
      "[3 rows x 12560 columns]\n"
     ]
    }
   ],
   "source": [
    "# Finding out which TF-IDF features contribute the most to determining the sentiment label\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "# Coefficients indicate the importance of each feature in the classification\n",
    "feature_importance = log_reg.coef_\n",
    "feature_importance_df = pd.DataFrame(feature_importance, columns=feature_names)\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_top_features = 500\n",
    "\n",
    "# Create a dictionary to store the top features for each sentiment label\n",
    "top_features_per_sentiment = {\n",
    "    -1: feature_importance_df.loc[0].nlargest(num_top_features).index.tolist(),\n",
    "    0: feature_importance_df.loc[1].nlargest(num_top_features).index.tolist(),\n",
    "    1: feature_importance_df.loc[2].nlargest(num_top_features).index.tolist()\n",
    "}\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    text = row['text']\n",
    "    sentiment_label = int(row['sentiment'])  # Convert sentiment label to integer\n",
    "    \n",
    "    if sentiment_label in top_features_per_sentiment:\n",
    "        top_features = top_features_per_sentiment[sentiment_label]  # All features for the sentiment label\n",
    "        sentiment_parts = [feature for feature in top_features if feature in text]\n",
    "    else:\n",
    "        sentiment_parts = []  # No top features available for the sentiment label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before dropna: 27480\n",
      "Number of rows after dropna: 27480\n",
      "       textID                                               text  sentiment  \\\n",
      "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh          0   \n",
      "1  96d74cb729   Shanghai is also really exciting (precisely -...          1   \n",
      "2  eee518ae67  Recession hit Veronique Branquinho, she has to...         -1   \n",
      "3  01082688c6                                        happy bday!          1   \n",
      "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!          1   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0                                   last session day  \n",
      "1  shanghai also really exciting precisely skyscr...  \n",
      "2  recession hit veronique branquinho quit compan...  \n",
      "3                                         happy bday  \n",
      "4                                               like  \n"
     ]
    }
   ],
   "source": [
    "#Applying preprocessing function on the text column\n",
    "test_df['cleaned_text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "# Converting sentiments to numerical form\n",
    "sentiment_mapping = {'neutral': 0, 'positive': 1, 'negative': -1}\n",
    "test_df['sentiment'] = test_df['sentiment'].map(sentiment_mapping)\n",
    "print(\"Number of rows before dropna:\", train_df.shape[0])\n",
    "\n",
    "test_df.dropna(inplace=True)\n",
    "print(\"Number of rows after dropna:\", train_df.shape[0])\n",
    "\n",
    "# Print the preprocessed test dataset\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1 -1 ...  0  1  0]\n"
     ]
    }
   ],
   "source": [
    "# Transform the preprocessed text data into feature vectors\n",
    "X_test = vectorizer.transform(test_df['cleaned_text']) \n",
    "\n",
    "# Use the trained model to predict the sentiment of the test dataset\n",
    "y_pred = log_reg.predict(X_test)  # Apply the trained model to the test dataset\n",
    "\n",
    "\n",
    "# Print the predicted sentiment labels for the test dataset\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty \"selected_text\" column in the test_df DataFrame\n",
    "test_df['selected_text'] = ''\n",
    "\n",
    "top_features_per_sentiment = {\n",
    "    -1: feature_importance_df.loc[0].nlargest(num_top_features).index.tolist(),\n",
    "    0: feature_importance_df.loc[1].nlargest(num_top_features).index.tolist(),\n",
    "    1: feature_importance_df.loc[2].nlargest(num_top_features).index.tolist()\n",
    "}\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    text = row['text']\n",
    "    sentiment_label = int(row['sentiment'])  # Convert sentiment label to integer\n",
    "    \n",
    "    if sentiment_label in top_features_per_sentiment:\n",
    "        top_features = top_features_per_sentiment[sentiment_label]  # All features for the sentiment label\n",
    "        sentiment_parts = [feature for feature in top_features if feature in text]\n",
    "    else:\n",
    "        sentiment_parts = []  # No top features available for the sentiment label\n",
    "    \n",
    "    # Assign the sentiment parts to the \"selected_text\" column\n",
    "    test_df.loc[index, 'selected_text'] = ' '.join(sentiment_parts)\n",
    "\n",
    "# Create the submission DataFrame with the desired columns\n",
    "submission_df = test_df[['textID', 'text', 'selected_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          textID                                               text  \\\n",
      "0     f87dea47db  Last session of the day  http://twitpic.com/67ezh   \n",
      "1     96d74cb729   Shanghai is also really exciting (precisely -...   \n",
      "2     eee518ae67  Recession hit Veronique Branquinho, she has to...   \n",
      "3     01082688c6                                        happy bday!   \n",
      "4     33987a8ee5             http://twitpic.com/4w75p - I like it!!   \n",
      "...          ...                                                ...   \n",
      "3529  e5f0e6ef4b  its at 3 am, im very tired but i can`t sleep  ...   \n",
      "3530  416863ce47  All alone in this old house again.  Thanks for...   \n",
      "3531  6332da480c   I know what you mean. My little dog is sinkin...   \n",
      "3532  df1baec676  _sutra what is your next youtube video gonna b...   \n",
      "3533  469e15c5a8   http://twitpic.com/4woj2 - omgssh  ang cute n...   \n",
      "\n",
      "           selected_text  \n",
      "0                     th  \n",
      "1          exciting g rs  \n",
      "2           shame e c es  \n",
      "3              happy day  \n",
      "4               pic like  \n",
      "...                  ...  \n",
      "3529  tired im sleep e c  \n",
      "3530    g ever live kiss  \n",
      "3531       nt dog e c es  \n",
      "3532              love g  \n",
      "3533          cute g pic  \n",
      "\n",
      "[3534 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(submission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
